# Vision Transformer (ViT)

This repository is an __unofficial implementation__ of Vision Transformer proposed in [here](https://arxiv.org/pdf/2010.11929.pdf).


## Requirements
1. torch
2. torchvision
3. tqdm


## Training
To run the training code, run the following code

```
python -m src.ViT.train
```

## Results
| Dataset | Accuracy | #Epochs |
|---------|----------|---------|
| MNIST   |          |         |

## References
1. Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).

## TODO:
- [ ] Add learned positional emebeddings
- [ ] Add CIFAR-10 results
